{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morning Lecture\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "__The goal of any model building process is to increase the predictive performance of a statistical model. But you must try to be sure that you aren't tricking yourself__\n",
    "\n",
    "__Bias__: tends to not change as much with different underlying sample\n",
    "__Variance__: more likely to vary as underlying sample changes\n",
    "\n",
    "We are not talking about a _particular_ model but the _procedure_ we use to obtain the model (e.g. logistic vs. sinusoidal)\n",
    "\n",
    "As complexity is increased - training error should always go down. However, test error is usually more U shaped. Test error is still the sum of bias and variance. But, when you see test and training error diverge you know you are in a high variance regime and could benefit from reducing complexity. When you see test and training error are both high you could benefit from increasing complexity.\n",
    "\n",
    "1. Randomize\n",
    "\n",
    "2. Split into training and validation\n",
    "\n",
    "3. True test set should only ever be looked at once. If you ever go back and redo the model you have compromised the test data and are underestimating the error.\n",
    "\n",
    "4. If you have enough data then split into three groups: training, validation, and testing. Leave test untouched and validate with validation.\n",
    "\n",
    "5. Even better: cross-validation. Split into k folds and use 1 fold as test and the rest as training. We don't technically have to have the testing data not overlap in all cases but we get a better result in the end if we do.\n",
    "\n",
    "6. Do not keep any of them and throw all of them away. Use this to __make decisions__. Cross validation is not for model optimization - it is for decision making.\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "Iterate through features and pick the best model you find. Start with simple model with one feature and train.\n",
    "\n",
    "__Never compare models based on R<sup>2</sup> (because it will always decrease with more features) or MSE on training set is essentially meaningless (guaranteed to be minimum for linear for example).__\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "Cross-validation does not make anything better on its own. It gives you data you didn't have.\n",
    "\n",
    "You could also do bootstrapped cross-validation.\n",
    "\n",
    "# Morning Breakout\n",
    "\n",
    "1. How can we increase the complexity of model? Add more features. Feature engineering is very important. Interaction terms are one thing multiplied by another thing.\n",
    "\n",
    "2. Why does variance increase and bias decrease as complexity increases? As you increase model complexity, you are increasing its flexibility. There will always be noise in the data that your model will be sensitive to if your model is very flexible.\n",
    "\n",
    "3. How do you determine optimal complexity? Make decisions with validation data and then when you finally report your estimated error you will use your test data, which you kept locked in a vault from the beginning and you don't go back and change anything after that.\n",
    "\n",
    "4. If you get low train error and high test error, what do you think is happening and what can you do about it? Can do something like backward stepwise selection for example: pick model with p-1 features that increases MSE the least. How do we compare model of p features, p-1 features, p-2 features, etc.? Cross-validation!!! F-test, adjusted R<sup>2</sup>, etc. are all just estimates of the training error that existed before we had the computing power to do validation, which we can better get from cross-validation.\n",
    "\n",
    "5. Why is k-fold better than simple train/test split? Each data point ends up in validation set and gives you better estimate of error.\n",
    "\n",
    "6. How many models created in 5-fold cross validation to create two models (A and B)? 10\n",
    "\n",
    "7. What do we do with al the models we create and how do we determine what single model to keep? We throw all those 10 models away and pick either procedure A or B. If we pick A then we train a new model A on _all_ of the data without sub-setting.\n",
    "\n",
    "# Afternoon Lecture\n",
    "\n",
    "Lee thinks anything that reduces features is regularization but others will only be referring to this specific thing when they say it. We will be modifying the cost function to add a term to penalize for extra parameters.\n",
    "\n",
    "__Normalize__: for each column, subtract mean and divide by standard deviation for each value. This is important for linear regression so that all the coefficients are on approximately the same scale.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
