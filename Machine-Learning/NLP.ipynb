{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TF-IDF\n",
    "TF-IDF, which stands for Term Frequency-Inverse Document Frequency, is a method for featurizing text which takes into account both word frequency within a single document, as well as the frequency of that word across all documents in a corpus. The purpose is to weight infrequent 'specialty' words more heavily than common words, since those specialty words tend to give us more information about the document.\n",
    "\n",
    "Words that appear in only a few documents give us a lot of information about the type of document it came from. \n",
    "#### This is a typical text featurization pipeline:\n",
    "```\n",
    "Tokenization ----> Stop Word Removal ----> Stemming/Lemmatization ----> Bag of words/TFIDF   \n",
    "```\n",
    "#### Tokenization\n",
    "As with preparing text for Naive Bayes, our pipeline starts by tokenizing the corpus into a list of list of strings. At this point, we should also make decisions about capitalization, punctuation, and whether to account for sentence and paragraph boundaries.\n",
    "\n",
    "#### Stop Words\n",
    "An ongoing theme in machine learning is that features that do not give a lot of information should be excluded or penalized. In NLP, the first step in reducing our feature-space is to remove stop words--words that appear frequently across many documents and across many topics. They're words that don't help us narrow down a document's unique qualities.\n",
    "\n",
    "#### Stemming and lemmatization\n",
    "\n",
    "- Stemming is a more crude technique that follows a set of rules for removing things like plurals, verb endings, and adjectivizations in order to condense many related words down to a single token. It will only ever remove or modify the ends of words, and a lot of the tokens it outputs are not actual words. Here is an example of a word passing through through the Porter Stemmer algorithm.\n",
    "```\n",
    "realizations --> realization --> realizate --> realiz\n",
    "```\n",
    "- Lemmatizers, on the other hand, have been trained to examine the morphology of each word, to determine its function in the sentence and modify it appropriately. A lemmatizer will generally output a larger feature set than a stemmer. In this case, lemmatizing 'realizations' yields 'realization'--not much of a change when compared to 'realiz'!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TF-IDF pseudo-code\n",
    "1. count the total number of documents in the corpus\n",
    "2. create a vocabulary\n",
    "3. create a matrix of zeroes of size num_docs * num_words_in_vocab\n",
    "4. for each word in the vocabulary:  \n",
    "tally number of documents in which the word appears   \n",
    "compute inverse document frequency   \n",
    "store this value   \n",
    "5. for each document in the corpus:   \n",
    "for each word in the document's bag of words:    \n",
    "tally raw term frequency   \n",
    "multiply term frequency by corresponding inverse document frequency   \n",
    "store this value at the appropriate location in the matrix   \n",
    "6. return the filled-in matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF\n",
    "\n",
    "$$idf=logâ¡(\\frac{N}{n_t+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Simiarity\n",
    "$$s(a,b) = \\frac{a\\cdot b}{||a|| ||b||} = \\frac{\\sum_{i=1}^N a_i b_i}{\\sqrt{\\sum_{i=1}^N a_i^2 }\\sqrt{\\sum_{i=1}^N a_i^2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# There's a function for each section of the sprint as well as some additional\n",
    "# helper functions.\n",
    "\n",
    "\n",
    "def main():\n",
    "    # GET THE DATA\n",
    "    # set categories = None to get all the data. Will be slow.\n",
    "    categories = ['comp.graphics', 'rec.sport.baseball', 'sci.med', \\\n",
    "                  'talk.politics.misc']\n",
    "    newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
    "\n",
    "    # DO TFIDF TRANSFORMATION\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vectors = vectorizer.fit_transform(newsgroups.data).toarray()\n",
    "\n",
    "    # FEATURE IMPORTANCES\n",
    "    top_n(vectorizer, vectors, newsgroups.data, 10)\n",
    "\n",
    "    # RANKING\n",
    "    ranking(vectorizer, vectors, newsgroups.filenames,\n",
    "            get_queries('queries.txt'), 3)\n",
    "\n",
    "    # SUMMARIZATION\n",
    "    article = newsgroups.data[1599]  # can choose any article\n",
    "    summarization(article, categories, 3)\n",
    "\n",
    "\n",
    "def top_n(vectorizer, vectors, data, n):\n",
    "    '''\n",
    "    Print out the top 10 words by three different sorting mechanisms:\n",
    "        * average tf-idf score\n",
    "        * total tf-idf score\n",
    "        * highest TF score across corpus\n",
    "    '''\n",
    "    words = vectorizer.get_feature_names()\n",
    "\n",
    "    # Top 10 words by average tfidf\n",
    "    # Take the average of each column, denoted by axis=0\n",
    "    avg = np.sum(vectors, axis=0) / np.sum(vectors > 0, axis=0)\n",
    "    print \"top %d by average tf-idf\" % n\n",
    "    print get_top_values(avg, n, words)\n",
    "    print\n",
    "\n",
    "    # Top 10 words by total tfidf\n",
    "    total = np.sum(vectors, axis=0)\n",
    "    print \"top %d by total tf-idf\" % n\n",
    "    print get_top_values(total, n, words)\n",
    "    print\n",
    "\n",
    "    # Top 10 words by TF\n",
    "    vectorizer2 = TfidfVectorizer(use_idf=False)\n",
    "    # make documents into one giant document for this purpose\n",
    "    vectors2 = vectorizer2.fit_transform([\" \".join(data)]).toarray()\n",
    "    print \"top %d by tf across all corpus\" % n\n",
    "    print get_top_values(vectors2[0], n, words)\n",
    "    print\n",
    "\n",
    "\n",
    "def get_queries(filename):\n",
    "    '''\n",
    "    Return a list of strings of the queries in the file.\n",
    "    '''\n",
    "    queries = []\n",
    "    with open('queries.txt') as f:\n",
    "        for line in f:\n",
    "            # horrible stuff to get out the query\n",
    "            queries.append(line.split(\"   \")[1].split(\"20\")[0].strip())\n",
    "    return queries\n",
    "\n",
    "\n",
    "def ranking(vectorizer, vectors, titles, queries, n):\n",
    "    '''\n",
    "    Print out the top n documents for each of the queries.\n",
    "    '''\n",
    "    tokenized_queries = vectorizer.transform(queries)\n",
    "    cosine_similarities = linear_kernel(tokenized_queries, vectors)\n",
    "    for i, query in enumerate(queries):\n",
    "        print query\n",
    "        print get_top_values(cosine_similarities[i], 3, titles)\n",
    "        print\n",
    "\n",
    "\n",
    "def summarize(article, sent_detector, n):\n",
    "    '''\n",
    "    Choose top n the sentences based on max tf-idf score.\n",
    "    '''\n",
    "    sentences = sent_detector.tokenize(article.strip())\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "    # We are summing on axis=1 (total per row)\n",
    "    total = np.sum(vectors, 1)\n",
    "    lengths = np.array([len(sent) for sent in sentences])\n",
    "    return get_top_values(total / lengths.astype(float), n, sentences)\n",
    "\n",
    "\n",
    "def summarization(article, categories, n):\n",
    "    '''\n",
    "    Print top n sentences from the article.\n",
    "    Print top n sentences from each category.\n",
    "    '''\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    # summarize an article\n",
    "    print summarize(article, sent_detector, n)\n",
    "    print\n",
    "\n",
    "    for cat in categories:\n",
    "        newsgroup = fetch_20newsgroups(subset='train', categories=[cat])\n",
    "        print cat\n",
    "        # combine all articles into one string to summarize.\n",
    "        print summarize(\"\\n\".join(newsgroup.data), sent_detector, n)\n",
    "\n",
    "\n",
    "def get_top_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the highest n values. Return\n",
    "    the labels for each of these indices.\n",
    "\n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
    "    output: [\"cat\", \"pig\"]\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[-1:-n-1:-1]]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
