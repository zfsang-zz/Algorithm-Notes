{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we do dimension reduction?\n",
    "- Combat the Curse Of Dimensionality  \n",
    "○ Remember how that ruined kNN and clustering?\n",
    "- Facilitate Visualization  \n",
    "○ Most people only want to see 2 or 3 dimensions at a time\n",
    "- Combine many raw features into a few meaningful latent features  \n",
    "- Remove Correlated Features  \n",
    "○ If you have p features, then you have p(p-1)/2 pairs of features. If p=10,000, that’s a lot.\n",
    "Many of those pairs will be correlated. \n",
    "- Compress your data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods of Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Subset selection of features; e.g. forward stepwise selection\n",
    "- LASSO regression\n",
    "- Relaxed LASSO:   \n",
    "○ (1) do LASSO regression, (2) throw away unused features, (3) do OLS regression\n",
    "- Neural networks: ○ (black box)   \n",
    "\n",
    "\n",
    "- Principal Components Analysis   (PCA)\n",
    "- Independent Component Analysis (ICA)\n",
    "- Canonical Correlation Analysis  (CCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors are directions in feature space (i.e., linear combinations of the original features), and the eigenvalues are the amount of variance captured along that direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvector with the largest eigenvalue is the direction along which the data set has the maximum variance. Meditate upon this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use when:\n",
    "- kNN on high dimensional data  \n",
    "- Clustering high dimensional data  \n",
    "- Visualization (e.g. embeddings)  \n",
    "- Working with images  \n",
    "(e.g. would it work well to feed an image into a decision tree?)\n",
    "\n",
    " \n",
    "Not Use when:\n",
    "- You need to retain interpretability of your feature space\n",
    "- Your model doesn’t need reduced dimensional data       \n",
    "(e.g. OLS on relatively small data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component  \n",
    "[2] https://www.cs.cmu.edu/~tom/10701_sp11/recitations/Recitation_11.pdf  \n",
    "[3] https://www.linkedin.com/pulse/dimension-reduction-pca-ica-cca-fld-topic-models-melenkevitz-phd  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Afternoon Lecture\n",
    "\n",
    "We got pretty uninterpretable results from SVD. The negative values in particular were annoying because you would have to look up corresponding U value for that negative value in V to know whether or not the ultimate result is positive or negative.\n",
    "\n",
    "Let's make everything positive instead!\n",
    "\n",
    "__Additive__ W and H only have positive values and so we know that the dot product will always be negative and only ever increase with additional terms.\n",
    "\n",
    "## Interpretation of topic-ness/latent features\n",
    "\n",
    "* __W__ Pick some number of the documents that have high values of topic. Look at those documents and try to figure out what topic is.\n",
    " - Each row in W is the amount that latent feature comprises each document\n",
    "\n",
    "\n",
    "* __H__ Pick some number of words that have high values of topic. Look at those words and try to figure out what that topic is.\n",
    " - Each column in H is the amount that word contributes to all latent features\n",
    "\n",
    "__Never yield a unique solution due to non-negativity constraint.__ This is the price we pay for interpretability. If you do multiple runs the qualities of each topic will probably remain the same but numbers will vary wildly.\n",
    "\n",
    "## Versus PCA and SVD\n",
    "\n",
    " - PCA and SVD decompose into three matrices\n",
    " - NMF only into two\n",
    "\n",
    " - __Non-negativity contraint__ is the main difference\n",
    "\n",
    " - __Bases__ are also not orthogonal like in PCA & SVD (also not even known and not technically an official vector basis)\n",
    "\n",
    "## How do we actually do this?\n",
    "\n",
    "### ALS: Alternating Least Squares\n",
    "\n",
    "Hold W constant and solve for best H we can. Enforce non-negativity by clipping all negative values to be 0.\n",
    "\n",
    "Hold H constant now and then solve for best W. Enforce non-negativity.\n",
    "\n",
    "* Fast, works well in practice\n",
    "\n",
    "* Non-negativity enforced in a weird ad-hoc way, not guaranteed to find a local minimum must less global, may not even converge (no convergence theory)\n",
    "\n",
    "* Have only reconstruction error as quantitative metric. PCA and SVD topics are ordered by importance so we know % explained variance of each topic and we know which ones are more important. We do not know this with NMF!\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "We are descending the gradient of cost function. So what is our cost function? __Forbenius norm of (X - WH)__. This is basically norm of squared difference between X and WH.\n",
    "\n",
    "Updates will get smaller and smaller (there is convergence theory).\n",
    "\n",
    "# Afternoon Breakout\n",
    "\n",
    "NMF is an unsupervised model!!\n",
    "\n",
    "NMF is a useful hammer.\n",
    "\n",
    "What are we modeling?\n",
    " - Latent features\n",
    " - It is doing this by trying to BEST describe (i.e. minimize reconstruction error) X with WH\n",
    "\n",
    "What hyperparameter choice must you make before performing NMF?\n",
    " - k\n",
    " - why would we choose one value of k over another? Domain knowledge? Prior constraint where you've already decided k?\n",
    "\n",
    "Why might we run NMF using same hyperparameter multiple times?\n",
    " - all the non-unique values will minimize the reconstruction error the same amount\n",
    " - however, pick the one that you like best. The most aesthetically pleasing one.\n",
    "\n",
    "Eg. NMF on Strava\n",
    " - with only one latent feature we would expect that LF to be 'power' (technically inverse power because smaller time means more powerful rider) because that is the best single value to attempt to reconstruct ride times for all riders\n",
    "\n",
    "Should we standardize?\n",
    " - Dividing by standard deviation is probably not awful but will probably not help\n",
    " - Subtracting mean will result in negative numbers which will fail\n",
    "\n",
    "Sparse matrices?\n",
    " - Single latent feature will be 0 if times are all 0 for that one rider\n",
    " - Still will have value for rider with times but will do a _bad job of reconstructing the 0s_\n",
    " - This is the issue with NMF and sparse matrices - will do bad reconstruction\n",
    "\n",
    "Moving rating utility matrix?\n",
    "- Audiences for movies??\n",
    "- W: users like certain 'topics' a certain amount\n",
    "- Many things about movies. Not just genre. Cast, producer, era, etc.\n",
    "\n",
    "Differs from k-means. NMF is a soft clustering technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.d.umn.edu/~mhampton/m4326svd_example.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
