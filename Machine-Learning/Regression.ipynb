{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "\n",
    "# Morning Lecture\n",
    "\n",
    "In linear regression we are trying to minimize the sum of square errors (i.e. e<sup>T</sup>e where e is a column vector of errors with same length as y).\n",
    "\n",
    "We assume independence of errors.\n",
    "\n",
    "e = (Y-Xβ)\n",
    "\n",
    "We want to minimize (Y-Xβ)<sup>2</sup> (this is cheating because this isn't how matrix calculus works). Take derivative of this with respect to β and set to 0.\n",
    "\n",
    "- -2X(Y-Xβ) = 0\n",
    "- X(Y-Xβ) = 0\n",
    "- X'Y - X'Xβ = 0\n",
    "- X'Y = X'Xβ\n",
    "- (X'X)<sup>-1</sup>X'Y = β\n",
    "\n",
    "Most computers are not going to solve this equation for OLS because it turns out finding the inverse of (X'X) is hard computationally and running gradient descent is easier.\n",
    "\n",
    "Residual sum of squares (RSS), as an absolute value is not very useful. Has to considered relative to the quantities you are trying to model. On the other hand, the residual standard error (RSE) tells us - on average - the error for each data point.\n",
    "\n",
    "Fraction of unexplained variance = 1 - ∑(y-yhat)<sup>2</sup>/∑(y-ybar)<sup>2</sup>\n",
    "\n",
    " - (y-yhat): unexplained deviation\n",
    " - (yhat-ybar): explained deviation\n",
    " - (y-ybar): total deviation\n",
    "\n",
    "How do we know if we should try adding another feature in? We could just try it; if it's useless its β will just be 0. We can also do an F-test to see if the β is 0.\n",
    "\n",
    "The constant typically loses its meaning at the y-intercept (i.e. when x is 0).\n",
    "\n",
    "If the coefficients are significant then most likely the F-statistic is also going to be significant. It's not really that useful.\n",
    "\n",
    "Recall: p-value is the probability of seeing test statistic the same or more extreme as the one observed - given that the null hypothesis β = 0 is true. If p-values are small then we reject the null and so the independent variable is probably useful.\n",
    "\n",
    "## Assumptions:\n",
    "\n",
    " - There is actual a linear relationship between ind and dep variables\n",
    "  * Can try a non-linear transformation of data\n",
    "\n",
    "\n",
    " - Constant variance (homoscedasticity)\n",
    "  * Can fix coning with log transformation of y\n",
    "\n",
    "\n",
    " - Independence of errors. This can trick you into thinking you're awesome at linear regression when you actually suck. __This is probably the most important assumption in linear regression.__\n",
    "  * If we have a time series then we add time step?\n",
    "\n",
    "\n",
    " - Normality of errors\n",
    "  * Outliers will violate this assumption\n",
    "\n",
    "\n",
    " - Lack of multicollinearity\n",
    "\n",
    "\n",
    "## Leverage\n",
    " - Linear regression will want to chase one very extreme outlier and try to fit it. That point is said to have a lot of _leverage_. THey are not necessarily bad, and may even be good data, but they have a lot of weight.\n",
    "\n",
    " - Leverage can sometimes not be a problem at all if it directly falls in line with the other data. It still has a lot of leverage but doesn't change the regression anyway.\n",
    "\n",
    " - Leverage\n",
    "\n",
    "## Multicollinearity\n",
    " - __Does not cause bias in model__\n",
    "\n",
    "With perfect multicollinearity, it is easy to detect because our model will fail to run.\n",
    "\n",
    "Can I predict another ind variable with a ind variable? If I can, then drop one.\n",
    "\n",
    "## QQ Plots\n",
    " 1. Make n predictions.\n",
    " 2. Take normal distribution and split it up into n chunks whose regions under curve are equal to 1/n\n",
    " 3. Match up my predictions with the normal distribution\n",
    " 4. This should give us straight line\n",
    "\n",
    "# Afternoon Lecture\n",
    "\n",
    "For categorical variables, create vector with 1 for 'yes' and 0 for 'no' and still follow same: Y = β<sub>0</sub> + βX<sub>male</sub> + βX<sub>female</sub>\n",
    "\n",
    "Cannot really include multiple categorical variables. In general, drop one of the options.\n",
    "\n",
    "We can lose some intuition if we add multiple categorical variables and drop options. E.g.\n",
    "\n",
    "Y = β<sub>0</sub> + βX<sub>male</sub> + βX<sub>caucasian</sub> + βX<sub>masters</sub>\n",
    "\n",
    "The constant β<sub>0</sub> should correspond to the 'baseline person'. This baseline person may not actually exist in the dataset in this case. Went from being (y-ybar) to (y-0).\n",
    "\n",
    "I get a benefit from increasing TV advertising but the existing radio advertising makes the effect of increased TV advertising even more. The combined effect is greater than the sum of parts! Our intuition is that people who see it from multiple mediums are more likely to buy.\n",
    "\n",
    "The most useful thing we can have domain knowledge - not just a fast computer. Being able to create features that best represent the real world will give us a much better model. Anybody can run gridsearch and try all possibilities. But only people who understand the underlying structure will be able to create features that represent the true relationship.\n",
    "\n",
    "__Gotta brush up on ma tractor knowledge__\n",
    "\n",
    "## Non-linear Features\n",
    "\n",
    "Just because relationship is non-linear, doesn't mean we can't use linear regression.\n",
    "\n",
    "It is good practice to put in regular variable term in along with polynomial term. E.g. you should put in TV and radio along with TV\\*radio.\n",
    "\n",
    "We are not only limited to transforming X. We can also transform Y.\n",
    "\n",
    "__Clumping is the worst case scenario for linear regression__\n",
    "\n",
    "If we see clumping, we want to make a transformation such that the data clump becomes a cloud.\n",
    "\n",
    "When we do these transformations, we no longer have easy to understand correlation: for a given 1 unit increase in x, we get β increase in y.\n",
    "\n",
    "## OLS stuff\n",
    "\n",
    "- Let's fit a simple model using the fancy formulas\n",
    "\n",
    "- Similar to R\n",
    "\n",
    "- Y ~ X means Y is somehow dependent on X\n",
    "    * 'Balance ~ Income + I(Age>60)' I is if you're going to transform variable here we're doing a boolean test for whether or not people are older than 60\n",
    "\n",
    "    * 'Balance ~ Income + Student + Income*Student. This is because we think that low income people who are students will have different spending than people who are actually low income\n",
    "\n",
    "    * 'Sales - TV*Radio-TV-Radio-1'. We must explicitly remove these otherwise OLS will put these in for you will see increase in R2 because we removed constant but it doesn't mean our model is actually better!\n",
    "\n",
    "- Which model is best? 2-degree, 3-degree, or linear?\n",
    "    * We want adjusted R squared, not regular R squared\n",
    "    * And t-statistic for betas\n",
    "\n",
    "\n",
    "- First try using engineered features of independent variables - so long as there no clumping in Y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression: L2 Regularization\n",
    "\n",
    "+ λ\\*Σβ<sub>i</sub><sup>2</sup> (not including β<sub>0</sub>)\n",
    "\n",
    "+ This new term shifts our cost function, which we want to minimize to find solution for βs\n",
    "\n",
    "If the regularization parameter (λ) is non-negative and not zero, we will increase our training error. The hope, however, is that this causes a reduction of our test error. The larger we make λ, the smaller the sum of β need to be.\n",
    "\n",
    "It is possible for optimization routine to trade off budget of βs between the individual βs and so it is not a _direct_ penalization of adding extra features.\n",
    "\n",
    "As we increase λ, we increase model's bias and decrease its variance. λ is a real value that we can choose. Our first hyperparameter!\n",
    "\n",
    "Tends to force parameters to be small. Computationally easier.\n",
    "\n",
    "## Lasso Regression: L1 Regularization\n",
    "\n",
    "+ λ\\*Σ|β<sub>i</sub>| (not including β<sub>0</sub>)\n",
    "\n",
    "+ This new term shifts our cost function, which we want to minimize to find solution for βs. Here we use absolute value instead of β<sub>i</sub><sup>2</sup>.\n",
    "\n",
    "Even though the value of a coefficient may become 0 and therefore, it makes it irrelevant to the model - this gives us a _different_ result than if we had just manually removed that column from the dataset because the _other_ coefficients are not the same.\n",
    "\n",
    "Tends to promote sparse models by setting coefficients exactly equal to 0. More computationally demanding than ridge.\n",
    "\n",
    "## Ridge vs Lasso\n",
    "\n",
    "For ridge coefficients asymptotically approach 0 while they actually become 0 for lasso. This is because derivative of L1 cost function does not exist at 0. The L1 normal has constant value slope for all values <0 and for all values >0 but there is no value at 0. On the other hand, L2 normal slope is constant line that passes through 0. (Side note, the idea of gradient descent is to move along this line for L2).\n",
    "\n",
    "In the case that λ is small the results for both lasso and ridge are almost the same because this is almost the same as not regularizing.\n",
    "\n",
    "When λ becomes large enough we are essentially predicting the mean because the coefficients all become 0 or become close enough to 0.\n",
    "\n",
    "sklearn calls λ alpha.\n",
    "\n",
    "All models are classes. Instances have fit() method, predict() method, and score() method.\n",
    "\n",
    "## Example\n",
    "\n",
    "1. Clean up X, etc.\n",
    "\n",
    "2. Decide we want to do linear regression.\n",
    "\n",
    "3. Linear regression too much so we decide to do lasso and ridge instead.\n",
    "\n",
    "4. Choose set of alphas to try\n",
    "\n",
    "5. Do k-fold CV on both ridge and lasso models for each alpha. (Ultimately train and validate k\\*number of alpha models)\n",
    "\n",
    "6. Look at mean scoring metric (i.e. average of k folds) for each of the models. Decide which alpha and model (ridge or lasso) was best.\n",
    "\n",
    "7. Throw away all these models. Take chosen model and .fit(X_train) <- on all of X data.\n",
    "\n",
    "8. Get result by scoring final trained model from step 7 on X_test. This is our estimate of what true error will be for model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
